{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction to Natural Language Processing (NLP)\n",
    "This Jupyter notebook was created for the 2018 AIS Natural Language Processing Workshop for the 2018 AI Conference at UT Dallas and focuses on using NLP techniques to build a sentiment classifier using data from 50,000 movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Natural Language Processing?\n",
    "Natural language processing, often abbreviated as NLP, is a broad area of artificial intelligence concerned with allowing machines to process and extract meaning from large amounts of human language data. There are many tasks in natural language processing, but in this workshop, we will be focusing primarily on feature extraction from text data and sentiment analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use this notebook\n",
    "This notebook has several cells with Python code in them. To run a cell, click on the cell and then press **SHIFT + ENTER**. You can also go to the \"Cell\" menu and select \"Run Cells\" but it is a bit faster and easier to just use the **SHIFT + ENTER** keyboard shortcut.\n",
    "\n",
    "### Installing the Libraries \n",
    "If you have the Anaconda distribution of Python, then you can install the necessary libraries using the following commands:\n",
    "\n",
    "- **conda install numpy**\n",
    "- **conda install pandas**\n",
    "- **conda install sklearn**\n",
    "- **conda install matplotlib**\n",
    "\n",
    "If you do not have the Anaconda distribution of Python installed, you can use pip to install the libraries above with commands such as **pip install numpy** for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # for linear algebra\n",
    "import pandas as pd # for CSV file I/O and data manipulation\n",
    "import sklearn # for machine learning\n",
    "import matplotlib.pyplot as plt # For making plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data - IMDb Movie Reviews Dataset\n",
    "We will be using a dataset of 50,000 movie reviews that was used in a Stanford paper titled \"Learning Word Vectors for Sentiment Analysis\". If you are interested, you can find the paper here: http://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf\n",
    "The original dataset, formatted as a CSV file, is included in the repository for this workshop, but you can also download the original dataset from http://ai.stanford.edu/~amaas/data/sentiment/ as a zip archive. Let's start by reading in the data as a dataframe object using the **read_csv** function from the **Pandas** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Leave it to Braik to put on a good show. Final...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nathan Detroit (Frank Sinatra) is the manager ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>To understand \"Crash Course\" in the right cont...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I've been impressed with Chavez's stance again...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This movie is directed by Renny Harlin the fin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0\n",
       "5  Leave it to Braik to put on a good show. Final...          1\n",
       "6  Nathan Detroit (Frank Sinatra) is the manager ...          1\n",
       "7  To understand \"Crash Course\" in the right cont...          1\n",
       "8  I've been impressed with Chavez's stance again...          1\n",
       "9  This movie is directed by Renny Harlin the fin...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('stanford_movie_data.csv') # pass in the name of the file\n",
    "data.head(10) # looks at the first ten rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, our dataset contains a series of reviews, with the sentiment of each review provided in the **sentiment** column. If we want to get some information about our dataframe, we can use the **info()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      "review       50000 non-null object\n",
      "sentiment    50000 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output above, we can confirm that our data contains 50,000 entries, with two columns, and we can even get an estimate of how much space our dataframe consumes in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## DataFrame Basics\n",
    "Pandas dataframes are really useful tools for working with data because we can easily query the data and perform operations on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Specific Columns\n",
    "We can use the following syntax to access specific columns of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        In 1974, the teenager Martha Moxley (Maggie Gr...\n",
       "1        OK... so... I really like Kris Kristofferson a...\n",
       "2        ***SPOILER*** Do not read this, if you think a...\n",
       "3        hi for all the people who have seen this wonde...\n",
       "4        I recently bought the DVD, forgetting just how...\n",
       "5        Leave it to Braik to put on a good show. Final...\n",
       "6        Nathan Detroit (Frank Sinatra) is the manager ...\n",
       "7        To understand \"Crash Course\" in the right cont...\n",
       "8        I've been impressed with Chavez's stance again...\n",
       "9        This movie is directed by Renny Harlin the fin...\n",
       "10       I once lived in the u.p and let me tell you wh...\n",
       "11       Hidden Frontier is notable for being the longe...\n",
       "12       It's a while ago, that I have seen Sleuth (197...\n",
       "13       What is it about the French? First, they (appa...\n",
       "14       This very strange movie is unlike anything mad...\n",
       "15       I saw this movie on the strength of the single...\n",
       "16       There are some great philosophical questions. ...\n",
       "17       I was cast as the Surfer Dude in the beach sce...\n",
       "18       I had high hopes for this one until they chang...\n",
       "19       Set in and near a poor working class town in t...\n",
       "20       Opulent sets and sumptuous costumes well photo...\n",
       "21       i saw the film and i got screwed, because the ...\n",
       "22       I'm getting a little tired of people misusing ...\n",
       "23       How offensive! Those who liked this movie have...\n",
       "24       What else can you say about this movie,except ...\n",
       "25       Certain aspects of Punishment Park are less th...\n",
       "26       First of all, I'd like to tell you that I'm in...\n",
       "27       You should not take what I am about to say lig...\n",
       "28       I love the Jurassic Park movies, they are thre...\n",
       "29       The first series of Lost kicked off with a ban...\n",
       "                               ...                        \n",
       "49970    Tom Fontana's unforgettable \"Oz\" is hands down...\n",
       "49971    Last weekend I bought this 'zombie movie' from...\n",
       "49972    I watched the first few moments on TCM a few y...\n",
       "49973    I saw this movie for the first time in 1988 wh...\n",
       "49974    Al Pacino? Kim Basinger? Tea Leoni? Ryan O'Nea...\n",
       "49975    Stanwyck at her villainous best, Robinson her ...\n",
       "49976    An allegation of aggravated sexual assault alo...\n",
       "49977    i thought this movie was wonderfully plotted i...\n",
       "49978    Just like most people, I couldn't wait to see ...\n",
       "49979    Dark comedy? Gallows humor? How does one make ...\n",
       "49980    ****Probably will contain spoilers****<br /><b...\n",
       "49981    I must be that one guy in America that didn't ...\n",
       "49982    THE PLOT: A trucker (Kristofferson) battles a ...\n",
       "49983    The Ladies Man is laugh out loud funny, with a...\n",
       "49984    Well, the artyfartyrati of Cannes may have lik...\n",
       "49985    The director was probably still in his early l...\n",
       "49986    You know when you're on the bus and someone de...\n",
       "49987    Five minutes into this movie you realize that ...\n",
       "49988    ...If you've been laughing too much for a long...\n",
       "49989    I love dissing this movie. My peers always try...\n",
       "49990    OK. I think the TV show is kind of cute and it...\n",
       "49991    Big disappointment. CLASH BY NIGHT is much to ...\n",
       "49992    Cassidy(Kacia Brady)puts a gun in her mouth bl...\n",
       "49993    With rapid intercutting of scenes of insane pe...\n",
       "49994    When \"Girlfight\" came out, the reviews praised...\n",
       "49995    OK, lets start with the best. the building. al...\n",
       "49996    The British 'heritage film' industry is out of...\n",
       "49997    I don't even know where to begin on this one. ...\n",
       "49998    Richard Tyler is a little boy who is scared of...\n",
       "49999    I waited long to watch this movie. Also becaus...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'] # Gets the review column of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        0\n",
       "2        0\n",
       "3        1\n",
       "4        0\n",
       "5        1\n",
       "6        1\n",
       "7        1\n",
       "8        1\n",
       "9        1\n",
       "10       0\n",
       "11       1\n",
       "12       0\n",
       "13       0\n",
       "14       1\n",
       "15       0\n",
       "16       0\n",
       "17       1\n",
       "18       0\n",
       "19       1\n",
       "20       0\n",
       "21       0\n",
       "22       0\n",
       "23       0\n",
       "24       0\n",
       "25       1\n",
       "26       0\n",
       "27       1\n",
       "28       0\n",
       "29       1\n",
       "        ..\n",
       "49970    1\n",
       "49971    0\n",
       "49972    1\n",
       "49973    1\n",
       "49974    0\n",
       "49975    1\n",
       "49976    0\n",
       "49977    1\n",
       "49978    0\n",
       "49979    1\n",
       "49980    0\n",
       "49981    0\n",
       "49982    0\n",
       "49983    1\n",
       "49984    0\n",
       "49985    0\n",
       "49986    0\n",
       "49987    0\n",
       "49988    0\n",
       "49989    0\n",
       "49990    0\n",
       "49991    0\n",
       "49992    0\n",
       "49993    1\n",
       "49994    1\n",
       "49995    0\n",
       "49996    0\n",
       "49997    0\n",
       "49998    0\n",
       "49999    1\n",
       "Name: sentiment, Length: 50000, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'] # Gets the sentiment column of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing/Slicing a Dataframe\n",
    "If we want to get a specific row of a dataframe, we can use the following syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       In 1974, the teenager Martha Moxley (Maggie Gr...\n",
       "sentiment                                                    1\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0] # Grabs the first row of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Leave it to Braik to put on a good show. Final...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nathan Detroit (Frank Sinatra) is the manager ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>To understand \"Crash Course\" in the right cont...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I've been impressed with Chavez's stance again...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This movie is directed by Renny Harlin the fin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I once lived in the u.p and let me tell you wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hidden Frontier is notable for being the longe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>It's a while ago, that I have seen Sleuth (197...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is it about the French? First, they (appa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This very strange movie is unlike anything mad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I saw this movie on the strength of the single...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>There are some great philosophical questions. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I was cast as the Surfer Dude in the beach sce...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I had high hopes for this one until they chang...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Set in and near a poor working class town in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Opulent sets and sumptuous costumes well photo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>i saw the film and i got screwed, because the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I'm getting a little tired of people misusing ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>How offensive! Those who liked this movie have...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What else can you say about this movie,except ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Certain aspects of Punishment Park are less th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>First of all, I'd like to tell you that I'm in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>You should not take what I am about to say lig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I love the Jurassic Park movies, they are thre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The first series of Lost kicked off with a ban...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>I first saw this movie on a local station on t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>I have read a lot of books in my short lifetim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>The Shining starts with Jack Torrance (Jack Ni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>The story is extremely unique.It's about these...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>I guess those who have been in a one-sided rel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Cates is insipid and unconvincing, Kline over-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>I am so excited that Greek is back! This seaso...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>I wanted to watch this, to get a inside look a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Imagine yourself trapped inside a museum of th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>SKELETON MAN was okay for the first 5 minutes ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>\"COSBY,\" in my opinion, is a must-see CBS hit!...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Just as the new BSG wasn't what fans of the or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>This is a big step down after the surprisingly...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>The only possible way to enjoy this flick is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>This is the kind of film they used to make, am...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Unlike some movies which you can wonder around...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>\"Dahmer\" is an interesting film although I wou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>This was obviously a low budget film. It shows...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>I just found the entire 3 DVD set at Wal-Mart ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>If you want to watch a film that is oddly shot...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review  sentiment\n",
       "0   In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1   OK... so... I really like Kris Kristofferson a...          0\n",
       "2   ***SPOILER*** Do not read this, if you think a...          0\n",
       "3   hi for all the people who have seen this wonde...          1\n",
       "4   I recently bought the DVD, forgetting just how...          0\n",
       "5   Leave it to Braik to put on a good show. Final...          1\n",
       "6   Nathan Detroit (Frank Sinatra) is the manager ...          1\n",
       "7   To understand \"Crash Course\" in the right cont...          1\n",
       "8   I've been impressed with Chavez's stance again...          1\n",
       "9   This movie is directed by Renny Harlin the fin...          1\n",
       "10  I once lived in the u.p and let me tell you wh...          0\n",
       "11  Hidden Frontier is notable for being the longe...          1\n",
       "12  It's a while ago, that I have seen Sleuth (197...          0\n",
       "13  What is it about the French? First, they (appa...          0\n",
       "14  This very strange movie is unlike anything mad...          1\n",
       "15  I saw this movie on the strength of the single...          0\n",
       "16  There are some great philosophical questions. ...          0\n",
       "17  I was cast as the Surfer Dude in the beach sce...          1\n",
       "18  I had high hopes for this one until they chang...          0\n",
       "19  Set in and near a poor working class town in t...          1\n",
       "20  Opulent sets and sumptuous costumes well photo...          0\n",
       "21  i saw the film and i got screwed, because the ...          0\n",
       "22  I'm getting a little tired of people misusing ...          0\n",
       "23  How offensive! Those who liked this movie have...          0\n",
       "24  What else can you say about this movie,except ...          0\n",
       "25  Certain aspects of Punishment Park are less th...          1\n",
       "26  First of all, I'd like to tell you that I'm in...          0\n",
       "27  You should not take what I am about to say lig...          1\n",
       "28  I love the Jurassic Park movies, they are thre...          0\n",
       "29  The first series of Lost kicked off with a ban...          1\n",
       "30  I first saw this movie on a local station on t...          1\n",
       "31  I have read a lot of books in my short lifetim...          0\n",
       "32  The Shining starts with Jack Torrance (Jack Ni...          1\n",
       "33  The story is extremely unique.It's about these...          1\n",
       "34  I guess those who have been in a one-sided rel...          1\n",
       "35  Cates is insipid and unconvincing, Kline over-...          0\n",
       "36  I am so excited that Greek is back! This seaso...          1\n",
       "37  I wanted to watch this, to get a inside look a...          1\n",
       "38  Imagine yourself trapped inside a museum of th...          0\n",
       "39  SKELETON MAN was okay for the first 5 minutes ...          0\n",
       "40  \"COSBY,\" in my opinion, is a must-see CBS hit!...          1\n",
       "41  Just as the new BSG wasn't what fans of the or...          1\n",
       "42  This is a big step down after the surprisingly...          0\n",
       "43  The only possible way to enjoy this flick is t...          0\n",
       "44  This is the kind of film they used to make, am...          1\n",
       "45  Unlike some movies which you can wonder around...          1\n",
       "46  \"Dahmer\" is an interesting film although I wou...          0\n",
       "47  This was obviously a low budget film. It shows...          0\n",
       "48  I just found the entire 3 DVD set at Wal-Mart ...          1\n",
       "49  If you want to watch a film that is oddly shot...          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0:50] # Grabs rows 0 to 49 of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Data Based on Conditions\n",
    "We can also select rows of our data based on certain conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Leave it to Braik to put on a good show. Final...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nathan Detroit (Frank Sinatra) is the manager ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>To understand \"Crash Course\" in the right cont...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I've been impressed with Chavez's stance again...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This movie is directed by Renny Harlin the fin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hidden Frontier is notable for being the longe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This very strange movie is unlike anything mad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I was cast as the Surfer Dude in the beach sce...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Set in and near a poor working class town in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Certain aspects of Punishment Park are less th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>You should not take what I am about to say lig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The first series of Lost kicked off with a ban...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>I first saw this movie on a local station on t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>The Shining starts with Jack Torrance (Jack Ni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>The story is extremely unique.It's about these...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>I guess those who have been in a one-sided rel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>I am so excited that Greek is back! This seaso...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>I wanted to watch this, to get a inside look a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>\"COSBY,\" in my opinion, is a must-see CBS hit!...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Just as the new BSG wasn't what fans of the or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>This is the kind of film they used to make, am...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Unlike some movies which you can wonder around...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>I just found the entire 3 DVD set at Wal-Mart ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>after my daughter was born in 1983, i needed t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Manna From Heaven is a light comedy that uses ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>I would love to comment on this film. Alas , m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>If Jean Renoir's first film \"Whirlpool of Fate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>I had the pleasure to view this film when I wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49922</th>\n",
       "      <td>Many movies try to take universal themes and m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49924</th>\n",
       "      <td>(WARNING: minor spoilers)&lt;br /&gt;&lt;br /&gt;I ran int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49926</th>\n",
       "      <td>This may be one of the best movies I have ever...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49930</th>\n",
       "      <td>Family Guy has to be my all time favorite cart...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49932</th>\n",
       "      <td>One of my favorite shows back in the '70s. As ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49933</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;Human Body --- WoW.&lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49939</th>\n",
       "      <td>\"Who Done It?\" contains many surefire laughs a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49940</th>\n",
       "      <td>I would have rated the series a perfect 10 for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49941</th>\n",
       "      <td>IMDb forces reviewers to type a certain amount...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49943</th>\n",
       "      <td>\"Ahh...I didn't order no amazing hit show\".......</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49945</th>\n",
       "      <td>I find this movie the best movie I have ever s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49949</th>\n",
       "      <td>The movie seemed a little slow at first. But i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49950</th>\n",
       "      <td>Okay, this film probably deserves 7 out of 10 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49958</th>\n",
       "      <td>Shtrafbat is the story only Russians could tel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49960</th>\n",
       "      <td>This is a phenomenal movie. Truly one of the b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49961</th>\n",
       "      <td>\"Men of honor\" - true story about a proud and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49962</th>\n",
       "      <td>While browsing the internet for previous sale ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49966</th>\n",
       "      <td>Typically, I'm a comedy guy. I rented this at ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49967</th>\n",
       "      <td>Who won the best actress Oscar for 1933? It sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49968</th>\n",
       "      <td>The rise of punk music was scarcely documented...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49970</th>\n",
       "      <td>Tom Fontana's unforgettable \"Oz\" is hands down...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49972</th>\n",
       "      <td>I watched the first few moments on TCM a few y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49973</th>\n",
       "      <td>I saw this movie for the first time in 1988 wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49975</th>\n",
       "      <td>Stanwyck at her villainous best, Robinson her ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49977</th>\n",
       "      <td>i thought this movie was wonderfully plotted i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49979</th>\n",
       "      <td>Dark comedy? Gallows humor? How does one make ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49983</th>\n",
       "      <td>The Ladies Man is laugh out loud funny, with a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49993</th>\n",
       "      <td>With rapid intercutting of scenes of insane pe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49994</th>\n",
       "      <td>When \"Girlfight\" came out, the reviews praised...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "3      hi for all the people who have seen this wonde...          1\n",
       "5      Leave it to Braik to put on a good show. Final...          1\n",
       "6      Nathan Detroit (Frank Sinatra) is the manager ...          1\n",
       "7      To understand \"Crash Course\" in the right cont...          1\n",
       "8      I've been impressed with Chavez's stance again...          1\n",
       "9      This movie is directed by Renny Harlin the fin...          1\n",
       "11     Hidden Frontier is notable for being the longe...          1\n",
       "14     This very strange movie is unlike anything mad...          1\n",
       "17     I was cast as the Surfer Dude in the beach sce...          1\n",
       "19     Set in and near a poor working class town in t...          1\n",
       "25     Certain aspects of Punishment Park are less th...          1\n",
       "27     You should not take what I am about to say lig...          1\n",
       "29     The first series of Lost kicked off with a ban...          1\n",
       "30     I first saw this movie on a local station on t...          1\n",
       "32     The Shining starts with Jack Torrance (Jack Ni...          1\n",
       "33     The story is extremely unique.It's about these...          1\n",
       "34     I guess those who have been in a one-sided rel...          1\n",
       "36     I am so excited that Greek is back! This seaso...          1\n",
       "37     I wanted to watch this, to get a inside look a...          1\n",
       "40     \"COSBY,\" in my opinion, is a must-see CBS hit!...          1\n",
       "41     Just as the new BSG wasn't what fans of the or...          1\n",
       "44     This is the kind of film they used to make, am...          1\n",
       "45     Unlike some movies which you can wonder around...          1\n",
       "48     I just found the entire 3 DVD set at Wal-Mart ...          1\n",
       "50     after my daughter was born in 1983, i needed t...          1\n",
       "52     Manna From Heaven is a light comedy that uses ...          1\n",
       "53     I would love to comment on this film. Alas , m...          1\n",
       "54     If Jean Renoir's first film \"Whirlpool of Fate...          1\n",
       "55     I had the pleasure to view this film when I wa...          1\n",
       "...                                                  ...        ...\n",
       "49922  Many movies try to take universal themes and m...          1\n",
       "49924  (WARNING: minor spoilers)<br /><br />I ran int...          1\n",
       "49926  This may be one of the best movies I have ever...          1\n",
       "49930  Family Guy has to be my all time favorite cart...          1\n",
       "49932  One of my favorite shows back in the '70s. As ...          1\n",
       "49933  <br /><br />Human Body --- WoW.<br /><br />The...          1\n",
       "49939  \"Who Done It?\" contains many surefire laughs a...          1\n",
       "49940  I would have rated the series a perfect 10 for...          1\n",
       "49941  IMDb forces reviewers to type a certain amount...          1\n",
       "49943  \"Ahh...I didn't order no amazing hit show\".......          1\n",
       "49945  I find this movie the best movie I have ever s...          1\n",
       "49949  The movie seemed a little slow at first. But i...          1\n",
       "49950  Okay, this film probably deserves 7 out of 10 ...          1\n",
       "49958  Shtrafbat is the story only Russians could tel...          1\n",
       "49960  This is a phenomenal movie. Truly one of the b...          1\n",
       "49961  \"Men of honor\" - true story about a proud and ...          1\n",
       "49962  While browsing the internet for previous sale ...          1\n",
       "49966  Typically, I'm a comedy guy. I rented this at ...          1\n",
       "49967  Who won the best actress Oscar for 1933? It sh...          1\n",
       "49968  The rise of punk music was scarcely documented...          1\n",
       "49970  Tom Fontana's unforgettable \"Oz\" is hands down...          1\n",
       "49972  I watched the first few moments on TCM a few y...          1\n",
       "49973  I saw this movie for the first time in 1988 wh...          1\n",
       "49975  Stanwyck at her villainous best, Robinson her ...          1\n",
       "49977  i thought this movie was wonderfully plotted i...          1\n",
       "49979  Dark comedy? Gallows humor? How does one make ...          1\n",
       "49983  The Ladies Man is laugh out loud funny, with a...          1\n",
       "49993  With rapid intercutting of scenes of insane pe...          1\n",
       "49994  When \"Girlfight\" came out, the reviews praised...          1\n",
       "49999  I waited long to watch this movie. Also becaus...          1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['sentiment'] == 1] # Gets all positive reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start working with NLP tools and actually getting into training machine learning algorithms, we need to preprocess the text data and remove unwanted characters such as HTML markup and punctuation. We can use Python's regex libary to do this. Just to show the an example of the effect of preprocessing, we can take a look at an entry in the review column before preprocessing and then after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I recently bought the DVD, forgetting just how much I hated the movie version of \"A Chorus Line.\" Every change the director Attenborough made to the story failed.<br /><br />By making the Director-Cassie relationship so prominent, the entire ensemble-premise of the musical sails out the window.<br /><br />Some of the musical numbers are sped up and rushed. The show\\'s hit song gets the entire meaning shattered when it is given to Cassie\\'s character.<br /><br />The overall staging is very self-conscious.<br /><br />The only reason I give it a 2, is because a few of the great numbers are still able to be enjoyed despite the film\\'s attempt to squeeze every bit of joy and spontaneity out of it.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, some of our reviews may have HTML characters and punctuation that we want to remove. Let's go ahead and define and run our preprocessing function on the review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re # regex library\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text) # Effectively removes HTML markup tags\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['review'] = data['review'].apply(preprocessor) #Applies the preprocessor function to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at this same entry in our preprocessed review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i recently bought the dvd forgetting just how much i hated the movie version of a chorus line every change the director attenborough made to the story failed by making the director cassie relationship so prominent the entire ensemble premise of the musical sails out the window some of the musical numbers are sped up and rushed the show s hit song gets the entire meaning shattered when it is given to cassie s character the overall staging is very self conscious the only reason i give it a 2 is because a few of the great numbers are still able to be enjoyed despite the film s attempt to squeeze every bit of joy and spontaneity out of it '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the same entry now has no HTML tages and punctuation. All of the characters have also been reduced to lowercase for uniformity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction from Text Data\n",
    "A key problem that appears in NLP applications that involve machine learning is extracting features from text data. In general, this problem involves converting text data into a set of quantitative values or features that summarize the data in a form that machine learning algorithms can actually work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Definitions\n",
    "Here are some terms that are frequently used throughout this tutorial that we should go ahead and define briefly before we proceed:\n",
    "- **document**: an ordered collection of characters or words that constitute a single instance of text data. Example: a single movie review can count as a document.\n",
    "- **corpus**: a usually large collection of documents that can be used for feature extraction techniques or generalizations on text data.\n",
    "- **n-gram**: a contiguous sequence of **n items** from a sample of text. These items can be words, characters, or even syllables depending on the application.\n",
    "- **unigram**: basically a single word or an **n-gram where n = 1**.\n",
    "- **bigram**: basically a sequence of two continguous words, or an **n-gram where n = 2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words - Looking at the Frequency of Words\n",
    "A popular and simple model for extracting features from text data is the **Bag of Words (BOW)** model. This model gets its name from the underlying assumption that each document can be treated as a **collection of words or n-grams** that does not take into account the order of the words but looks at the frequency of each word. \n",
    "\n",
    "This model is also called the **vector-space model** because it transforms each document in a corpus into a vector, where each component corresponds to the frequency of a particular word from the whole corpus in that specific document. This process is known as **count vectorization**. Let's go through an example to demonstrate how this process works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer # import CountVectorizer module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported the CountVectorizer module, let's go ahead and fit a count vectorizer on a small corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = 'I like cats'\n",
    "doc2 = 'I like dogs and cats'\n",
    "doc3 = 'The cats got the rats'\n",
    "corpus = [doc1, doc2, doc3] # the corpus is basically a list of strings (documents)\n",
    "\n",
    "count_vectorizer = CountVectorizer() # Creates a CountVectorizer with default parameters\n",
    "count_vectorizer.fit(corpus) # Fits the count vectorizer on the corpus we just created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a CountVectorizer object that has been trained on a corpus, there are several operations that we can perform with it. First of all, let's take a look at the our count vectorizer's vocabulary - all of the unique words that the vectorizer found in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0, 'cats': 1, 'dogs': 2, 'got': 3, 'like': 4, 'rats': 5, 'the': 6}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, our vectorizer has seven words in its vocabulary. Single letter words such as \"I\" are not considered by default. The vocabulary is represented as a dictionary where the keys are the words or n-grams (a word is basically a 1-gram) and the values are the index of each word for the vectors of each document. To demonstrate this concept, let's actually convert some sentences to vectors using the **transform** function. Let's transform the first document into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like cats'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x7 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.transform([doc1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to transform the first document like this, notice how we get a **sparse matrix**. This is because in general, these vectors can become quite large, with many zeros when using a corpus with many words. Our corpus is really small but the transform function still outputs the vector in this format. If we want to actually see the vector, we can convert the sparse matrix to a dense matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.transform([doc1]).todense() # Allows us to see the actual values in the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vector basically tells us that the words **\"cat\"** and **\"like\"** both appear once in doc1 and the other words in the vocabulary do not appear at all. We can actually take any sentence or document and transform it into a vector with our count vectorizer. **The vectorizer ignores words that are not in its vocabulary.** Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.transform(['I like cats and dogs, but not rats.']).todense() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 2, 1, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.transform(['Dogs are cool, but cats are scary! I do not like cats']).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams and Bag of Words\n",
    "In the previous examples, we were using the count vectorizer with just unigrams or single words. We can adjust the range of n-grams that our vectorizer considers to get different results. The **ngram_range** parameter allows us to pass in a tuple specifying the range of n-grams to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer_bigram = CountVectorizer(ngram_range=(1,2)) # This will consider unigrams and bigrams\n",
    "count_vectorizer_bigram.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'and cats': 1,\n",
       " 'cats': 2,\n",
       " 'cats got': 3,\n",
       " 'dogs': 4,\n",
       " 'dogs and': 5,\n",
       " 'got': 6,\n",
       " 'got the': 7,\n",
       " 'like': 8,\n",
       " 'like cats': 9,\n",
       " 'like dogs': 10,\n",
       " 'rats': 11,\n",
       " 'the': 12,\n",
       " 'the cats': 13,\n",
       " 'the rats': 14}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer_bigram.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how our vectorizer now has a larger vocabulary because it not only considers single words (unigrams) but also instances of two adjacent words (bigrams). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF: A More Sophisticated Bag of Words Model\n",
    "So far we have only worked with a simple bag of words model where we just count the frequencies of each vocabulary word in each document. However, one problem with this simple approach is that for a large corpus, words that frequently appear in the English language will have larger components when in reality, they do not contribute much to the overall meaning of each document. \n",
    "\n",
    "**TF-IDF**, short for **term frequency-inverse document frequency** is another bag of words approach that attempts to solve this problem. Rather than just computing the raw frequencies of each word in a document, the TF-IDF approach involves multiplying these frequencies by the **inverse** of a statistic representing the **frequencies of these words in the entire corpus**. The TF-IDF statistic is basically the **product** of the **term frequency** and **inverse document frequency** statistics as demonstrated in the equation below:\n",
    "\n",
    "\\begin{equation*}\n",
    "TFIDF(t, d, D) = f_{t, d} \\bullet log\\frac{N_D} {n_t}\n",
    "\\end{equation*}\n",
    "\n",
    "Where:\n",
    "- $t$, $d$, and $D$ are the **term (word or n-gram)**, **document**, and **corpus** respectively.\n",
    "- $f_{t,d}$ is the **term-frequency statistic**, which is just the **raw frequency** of the term $t$ in the document $d$.\n",
    "- $N_D$ is the **total number of documents** in the corpus $D$.\n",
    "- $n_t$ is the **number of documents** in which the term $t$ appears.\n",
    "\n",
    "Notice that if a word appears in **every document**, which means $N_D = n_t$, then the TF-IDF statistic for that word will be zero because in that case $\\frac{N_D} {n_t} = 1$ and $log(1) = 0$. This means words in the English language that are very common but do not contribute much to the meaning of the documents will likely have near-zero TF-IDF scores. These words are often called **stop words**. One of the reasons for using this logarithm in the equation is that **stop words** will usually receive scores that are close to, if not equal to zero.\n",
    "\n",
    "#### Running TF-IDF on our small corpus\n",
    "To demonstrate how TF-IDF differs from the simpler Bag of Words approach with just word frequencies, let's try fitting a TFIDF vectorizer on our corpus and transforming the same documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like cats'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.61335554, 0.        , 0.        , 0.78980693,\n",
       "         0.        , 0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.transform([doc1]).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we get floating point values rather than integer values for the components corresponding to the words \"cat\" and \"like\". Here are some more examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.50461134, 0.29803159, 0.50461134, 0.        , 0.38376993,\n",
       "         0.50461134, 0.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.transform(['I like cats and dogs, but not rats.']).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.68499287, 0.57989687, 0.        , 0.44102652,\n",
       "         0.        , 0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.transform(['Dogs are cool, but cats are scary! I do not like cats']).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis - Combining Text Transformation and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have seen how to extract feature vectors from text data, we can start training machine learning models using those feature vectors.\n",
    "\n",
    "### Train, Test, Split\n",
    "Before we start training machine learning models on the data, we should split our data into a **training set** and a **testing set**. We can do this using SciKit-Learn's **train_test_split** tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data['review']\n",
    "y = data['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's going on in the code above?\n",
    "We did the following:\n",
    "\n",
    "- Created a dataframe **X** to represent the **features of the data** that we will train the model on.\n",
    "- Created a dataframe **y** to represent the **target variable** (whether the review is positive or negative).\n",
    "- Created four dataframes using train_test_split:\n",
    "    - **X_train**: represents the features in training data.\n",
    "    - **X_test**: represents the features in testing data.\n",
    "    - **y_train**: represents the target values in training data.\n",
    "    - **y_test**: represents the target values in testing data.\n",
    "    \n",
    "### How train_test_split works\n",
    "\n",
    "The train_test_split function takes our features (X) and our target values (y) and splits them into training and testing sets.\n",
    "- **test_size**: Represents the proportion of the data that will be used to test the data.\n",
    "- **random_state**: Just a random seed for randomly performing the split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes for Sentiment Classification\n",
    "Naive bayes is a very simple classification algorithm with fast training times and generally good performance for text classification tasks. It involves applying Bayes' Theorem with the \"naive\" assumption that all of the features are independent from each other. Naive bayes is based on the concept of conditional probability. Given an instance of $n$ features $x = (x_1, ... x_n)$ and a set of $k$ classes $C = (C_1, ..., C_k)$, this algorithm assigns conditional probabilities in the following form for each of the $k$ classes:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(C_k \\space| \\space (x_1, ..., x_n))\n",
    "\\end{equation*}\n",
    "\n",
    "The expression above basically represents the probability of the class $C_k$ given the features in $x$. In the context of our sentiment analysis problem, there are two classes (1 and 0) where one represents a positive review and the other represents a negative review. The feature vector $x$ for this problem is basically the TF-IDF vector with the scores for each word in the corpus for a particular review. \n",
    "\n",
    "Bayes' Theorem, with reference to this conditional probability is:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(C_k \\space| \\space x) = \\frac{p(C_k)p(x \\space | \\space C_k)}{p(x)}\n",
    "\\end{equation*}\n",
    "\n",
    "Basically, this theorem, in the context of our problem, states that the probability of a review being belonging to a specific class $C_k$ (ie. positive or negative) is equal to the probability of the class $C_k$ multiplied by the probability of the TF-IDF vector $x$ given that it belongs to the class $C_k$ divided by the probability of the TF-IDF vector $x$.\n",
    "\n",
    "We can expand the conditional probability of the TF-IDF vector as a product of each of the conditional probabilities of each of its components with respect to the class $C_k$. Let's assume that the vector $x$ has the components $x_1$, $x_2$, ... $x_n$. Then our equation becomes:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(C_k \\space | \\space x) = \\frac{p(C_k)p(x_1 \\space | \\space C_k)p(x_2 \\space | \\space C_k)...p(x_n \\space| \\space C_k)}{p(x_1)p(x_2)...p(x_n)}\n",
    "\\end{equation*}\n",
    "\n",
    "We can expand $p(x)$ and $p(x \\space | \\space C_k)$ as products because naive bayes is based on the \"naive\" assumption that **each of our features are independent**.\n",
    "\n",
    "Given these equations and a TF-IDF vector for a given review, the naive bayes algorithm involves calculating the conditional probability of each possible class given the features in the vector and then selecting the **class with the highest conditional probability**.\n",
    "\n",
    "#### Calculating the probability of each feature\n",
    "One question that remains in our discussion of this algorithm is how can we can actually calculate the probability of each feature in the TF-IDF vector? There are a few different versions of naive bayes that use different probability distributions to calculate the probability of each feature.\n",
    "\n",
    "#### Gaussian Naive Bayes - The Normal/Gaussian Distribution\n",
    "Gaussian naive bayes assumes that each feature belongs to a **normal** or **gaussian** distribution. The behavior of the normal distribution is determined by two parameters, the **mean** and the **standard deviation**. A standard normal distribution has a **mean of 0** and a **standard deviation of 1**. The standard normal distribution can be graphed using the following equation (the PDF or probability density function of the distribution):\n",
    "\n",
    "\\begin{equation*}\n",
    "f(z) = e^{(\\frac {-z^2}{2})}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x116f34550>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUsoLabkrpZQX0nJXSikv9P8ucXGWN9sAnAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116f08588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z_values = np.linspace(-3,3,num=1000)\n",
    "function_values = np.exp(-1 * (z_values **2 )/2) # Plotting the function above\n",
    "plt.plot(z_values, function_values)\n",
    "plt.title(\"The Standard Normal Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to scale this curve to data without a mean of 0 and a standard deviation of 1, we can take each observation $ x $ and calculate the z-score $z$ based on the mean $\\mu$ and the standard deviation $\\sigma$. \n",
    "\n",
    "\\begin{equation*}\n",
    "z = \\frac {x - \\mu} {\\sigma}\n",
    "\\end{equation*}\n",
    "\n",
    "The resulting PDF, which results from substituting the expression above for z, is as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "f(x) = e^{{- (\\frac {x - \\mu} {2\\sigma})^2}}\n",
    "\\end{equation*}\n",
    "\n",
    "We can calculate the probability of a range of values by finding the area under the curve for this range of values. Because the probability of a single value is not defined in such a distribution, we can split the distribution into ranges or quantiles and find the probability of the a value fitting into a specific range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes - using the multinomial distribution\n",
    "This variant of naive bayes is commonly used in text classification and assumes that the features belong to a multinomial distribution. The distribution is parameterized by the vectors $ \\theta_{y} = (\\theta_{y1}, ... \\theta_{yn})$ where $n$ is the number of features (in our case the size of the vocabulary) and $y$ is the class for which we are computing the vector, and $\\theta_{yi}$ is the probability $p(x_i | y)$ of feature $i$ appearing in a sample belonging to the class $y$.\n",
    "\n",
    "The parameters $\\theta_{y}$ are estimated using the following equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat \\theta_{yi} = \\frac {N_{yi} + \\alpha} {N_{y} + \\alpha n}\n",
    "\\end{equation*}\n",
    "\n",
    "Equation source: SciKit-Learn documentation: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "Where $N_{yi}$ is **the number of times** feature $i$ appears **in a sample** of class $y$ in the training data and $N_{y}$ is the **total count of all features** for class $y$. The parameter $\\alpha$ is a **smoothing parameter** that is used to prevent zero probabilities for features not present in learning samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bernoulli Naive Bayes - using the Bernoulli distribution \n",
    "Bernoulli naive bayes is based on the assumption that the features belong to a Bernoulli distribution. This distribution treats each feature as a binary-valued (boolean or Bernoulli) variable. The possible outcomes are essentially 1 (success) or 0 (failure) and the distribution is parameterized by a probability $p$ of success. Since TF-IDF vectors do not contain binary features, a Bernoulli naive bayes classifier will automatically binarize these features by dividing the feature range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining TF-IDF and Naive Bayes Using a Pipeline\n",
    "Here we will combine the TF-IDF vectorizer and the naive bayes algorithm into a single classifier using a SciKit-Learn's Pipeline module. We will then train this classifier on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB # We will be using Multinomial Naive Bayes here\n",
    "from sklearn.pipeline import Pipeline # This module allows us to chain different operations together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...     vocabulary=None)), ('naive_bayes', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_pipeline = Pipeline([('tfidf', TfidfVectorizer()), ('naive_bayes', MultinomialNB())])\n",
    "nb_pipeline.fit(X_train, y_train) # Fits the classifier on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our TF-IDF/Naive Bayes Algorithm\n",
    "Now that we've trained our sentiment classifier on the training data, we can go ahead and evaluate it using the test data. We will first have our classifier generate predictions on the test features (**X_test**) and then compare these predictions to the correct labels (**y_test**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score # Metrics for evaluating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.85      0.87      7821\n",
      "          1       0.85      0.89      0.87      7179\n",
      "\n",
      "avg / total       0.87      0.87      0.87     15000\n",
      "\n",
      "Accuracy on test set: 86.84666666666668 %\n"
     ]
    }
   ],
   "source": [
    "pred = nb_pipeline.predict(X_test) # Generates predictions on the test data\n",
    "print(classification_report(pred, y_test)) # generates a classification report with several metrics\n",
    "print('Accuracy on test set: {} %'.format(100*accuracy_score(pred, y_test)))  # prints out the accuracy on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with a relatively simple naive bayes approach, our classifier achieved over 86% accuracy on the test set. Here is what the other metrics mean:\n",
    "\n",
    "- **precision**: the ratio of correctly predicted positive observations (true positives) to the total predicted positive observations. For example in the row labeled 1 above, this is the number of times our model correctly predicted a positive review divided by the total number of times our model predicted a positive review.\n",
    "- **recall**: the ratio of correctly predicted positive observations to all observations actually in that class. For example, in row 1, this is number of times our model correctly predicted a positive review divided by the actual number of positive reviews.\n",
    "- **f1-score**: the harmonic mean of precision and recall. The harmonic mean is often roughly equivalent to the average of two values in most cases.\n",
    "\n",
    "The f1-score is given by the following equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "F_{1} = 2 \\bullet \\frac{precision \\bullet recall} {precision + recall}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using K-Fold Cross Validation\n",
    "Cross validation is a technique for evaluating model performance that is similar to what we have been doing so far. A common form of cross validation, known as **k-fold cross validation**, involves splitting a dataset into a single multiple sections called **folds** (k folds to be exact!) and in a single iteration, using **k-1 folds for training** and just **one fold for testing**. This process is then repeated **k times** with a different fold being used as test data each time. The values of the evaluation metrics for each **iteration** can then be averaged to get a more robust measure of a model's performance. This practice is better than using the same test set again and again because optimizing a model's performance on one specific test set in effect makes that test set a part of the training set. \n",
    "\n",
    "### SciKit-Learn's cross_val_score function\n",
    "Fortunately, SciKit-Learn has a very convenient function for computing the cross validation scores for a given model. This function returns a list (array) with the cross validation scores for the model. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86459083, 0.8625345 , 0.86091444])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score # Module for cross validation\n",
    "cross_val_score(nb_pipeline, X, y, scoring='accuracy', cv=3) # The cv parameter is the number of folds/iterations to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our model on raw text data\n",
    "Just to convince ourselves that our classifier actually works, we can try testing it on raw text reviews that we can make up on the spot. Our classifier will return a Numpy array with a single value of 1 or 0 depending on whether it predicts that the review is positive or negative respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_pipeline.predict(['That was a great movie!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_pipeline.predict(['That was a terrible movie!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression for Sentiment Classification\n",
    "Unlike what the name suggest, **logistic regression** is designed specifically for **classification** and is especially suited for **binary classification** problems. The real **\"regression\"** in logistic regression involves finding the best values in a weight vector that is used to compute a weighted sum of the input features. This weighted sum is then passed to a function called the **sigmoid function** also known as the **logistic function**. This function has a special property where it takes any real-valued input and outputs a continuous value between 0 and 1. In general, if the output of this function is below 0.5 we can assign the input to class 0 and if the output is greater than or equal to 0.5, we can assign the input to class 1.\n",
    "\n",
    "### How Logistic Regression Works\n",
    "The **logistic function** is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\phi (z) = \\frac {1} {1+e^{-z}}\n",
    "\\end{equation*}\n",
    "\n",
    "To better illustrate the concept of the **logistic function**, the function has been plotted below in the interval [-10, 10]. Notice how the function forms an S-shaped curve with horizonal asymptotes at y = 0 and y = 1. What's special about this function is that it can take any real value and output a value between 0 and 1, which is really useful for representing probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1142f0d68>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHAxJREFUeJzt3XuUVOWd7vHvz25AUBQiiIigxEEiifcOasYLjhoBYzBy\nQPASYowuMhpnJpM5kouezEomUSdm1AWGgBpRE7lEbjEgeBvNeEEbRxAQFEUUEGxRNIACbf/OH28R\nyraru7p7V79Vu57PWnvVrtqbrofd3Q+bt/bF3B0REUmXvWIHEBGR5KncRURSSOUuIpJCKncRkRRS\nuYuIpJDKXUQkhVTuIiIppHIXEUkhlbuISApVxnrjbt26+WGHHRbr7UVEStLixYvfdffuTa0XrdwP\nO+wwqqurY729iEhJMrO1+aynYRkRkRRSuYuIpJDKXUQkhVTuIiIppHIXEUmhJsvdzO4ys3fMbFmO\n5WZmt5nZajNbambHJx9TRESaI58997uBwY0sHwL0y0xXAr9pfSwREWmNJo9zd/cnzeywRlYZBtzj\n4X59z5pZFzPr6e5vJ5RRRFKqthY+/hh27AiPO3eG+Z07YdeuPY+1tZ+ePvnks1Nd3Z7H+pP7p+dz\nTZD//G657lTa2DqnnAJf/Wrrt19jkjiJqRfwVtbzdZnXPlPuZnYlYe+ePn36JPDWIhJLXR3U1MCm\nTeGxpgY2b4b33w/TBx+E6cMP4a9/ha1bYdu2MH30EWzfHoq6XJjtmb/22tIo97y5+yRgEkBVVZXu\nzC1SxNxh/XpYtQpeeQXWrIG1a8O0fj1s3Ji7nDt1gi5dYP/9Yb/9YN99oUeP8NipU5g6dgzT3nuH\nqX176NAhTO3bh6ldO6isDI/t2kFFRXheUdHwtNdeYdo9b7bntd3zZo1PkP/8btnzxSKJcl8P9M56\nfkjmNREpEbW1sHQpLFoES5aE6aWXwl72bu3bQ58+cOihcNZZcPDB0LMnHHQQdO8epgMOgK5dw7oS\nVxLlPhe42symAicCH2i8XaS41dbC88/DI4/AY4/Bc8+FYRII5Xz00fDtb8ORR0L//tCvH/TqFfZ8\npTQ0We5mdj8wCOhmZuuA/we0A3D3icA8YCiwGtgOXFaosCLSch99BA89BA88AA8+GMbDzeC44+A7\n34GTT4aTTgp75sU4zCDNk8/RMqObWO7AVYklEpHEuMMzz8DkyTBjRhhmOeAAuOACGDIEzjgDunWL\nnVIKIdolf0WkcHbsgHvugVtugRUrwgeZo0fDqFFw+unhQ0lJN32LRVJk2zb47W/h5pthwwY4/ni4\n4w648MJQ8FI+VO4iKVBXB3/4A4wbFw5TPOMMmDIFzjxT4+flSuUuUuIWL4arrgqHMVZVwf33w6mn\nxk4lsenAJpEStWsX/PSncOKJ4cSiu+8OBa9iF9Ceu0hJWrkSLrkk7LVfcgncdls4Pl1kN5W7SIn5\n05/g4ovDafoPPBAOaxSpT8MyIiXCHX75Sxg2DI44Av73f1Xskpv23EVKQG0tXH55OHZ99Gi4885w\n0S2RXLTnLlLkdu2Ciy4Kxf7v/w6//72KXZqmPXeRIrZjRzgBac6ccGLS978fO5GUCpW7SJGqrYUR\nI8IHqOPHh2PZRfKlchcpQu7wve+FYp8wAf7xH2MnklKjMXeRInTTTTBxYricgIpdWkLlLlJkpk4N\npT5qFPzHf8ROI6VK5S5SRJYtg8suC5cQuPtu3flIWk4/OiJFYts2GDky3FR6xoxwBqpIS+kDVZEi\ncdVV4ZoxDz8MPXrETiOlTnvuIkVgypQw/eQn4RrsIq2lcheJbO3asNd+2mlw/fWx00haqNxFInKH\n7343zN9zj+5tKsnRj5JIRFOnwvz54UbWhx4aO42kifbcRSJ591245hoYOBCuvjp2GkkblbtIJP/6\nr7BlC9xxB1RUxE4jaaNyF4ng6afDGPu118JRR8VOI2mkchdpY+5hr71nT/jhD2OnkbTSB6oibWzG\nDHj22XA3pX32iZ1G0kp77iJtaMeOcFGwo4+GMWNip5E00567SBsaPx7WrIGFC/UhqhSW9txF2sj7\n78PPfw6DB8PZZ8dOI2mXV7mb2WAzW2Vmq81sXAPL9zezP5nZEjNbbmaXJR9VpLTddls49PGXv4yd\nRMpBk+VuZhXABGAIMAAYbWYD6q12FbDC3Y8BBgE3m1n7hLOKlKwPPwxnoQ4bBsceGzuNlIN89twH\nAqvd/XV33wlMBYbVW8eBzmZmwL7Ae0BtoklFStj48WGv/brrYieRcpFPufcC3sp6vi7zWrbxwJHA\nBuAl4J/cvS6RhCIlbutW+PWvYcgQOOGE2GmkXCT1geo5wIvAwcCxwHgz26/+SmZ2pZlVm1l1TU1N\nQm8tUtwmToTNm7XXLm0rn3JfD/TOen5I5rVslwEzPVgNrAG+UP8Lufskd69y96ru3bu3NLNIyfjo\nI/jVr+Css+Dkk2OnkXKST7k/D/Qzs76ZD0lHAXPrrfMmcCaAmfUA+gOvJxlUpBTddx9s2gQ/+lHs\nJFJumjyJyd1rzexqYAFQAdzl7svNbGxm+UTgZ8DdZvYSYMC17v5uAXOLFD13uPXWcHTMoEGx00i5\nyesMVXefB8yr99rErPkNwFeTjSZS2h59FJYvh9/9Dsxip5FyozNURQrkllvgwANh1KjYSaQcqdxF\nCuDVV+HPf4axY2HvvWOnkXKkchcpgNtug3bt9tz8WqStqdxFEvbBB3D33WE45qCDYqeRcqVyF0nY\nffeFs1KvuSZ2EilnKneRBLnD5Mlw3HFQVRU7jZQzlbtIgqqrYckSuOKK2Emk3KncRRI0eTJ06gQX\nXRQ7iZQ7lbtIQrZuhfvvh5EjYf/9Y6eRcqdyF0nItGmh4DUkI8VA5S6SkMmT4cgjdfVHKQ4qd5EE\nLFsGixaFvXZdR0aKgcpdJAFTpkBlJVx6aewkIoHKXaSVPvkE/vAHGDoUunWLnUYkULmLtNLjj8OG\nDXDJJbGTiOyhchdppfvug/32g699LXYSkT1U7iKtsH07PPAAjBgBHTvGTiOyh8pdpBXmzAnHtmtI\nRoqNyl2kFe69F3r3htNOi51E5NNU7iIttGkTLFwIF18Me+k3SYqMfiRFWmj69HAY5MUXx04i8lkq\nd5EWmjYNvvSlMIkUG5W7SAusWwdPPQUXXhg7iUjDVO4iLfDHP4bHESPi5hDJReUu0gLTp8Mxx0D/\n/rGTiDRM5S7STG++Cc88oyEZKW4qd5FmmjEjPI4cGTeHSGNU7iLNNH06nHACHH547CQiuancRZph\nzRp47jnttUvxU7mLNMPuIRkdJSPFTuUu0gwzZ4Yhmb59YycRaVxe5W5mg81slZmtNrNxOdYZZGYv\nmtlyM3si2Zgi8a1bF+6TesEFsZOINK2yqRXMrAKYAJwNrAOeN7O57r4ia50uwO3AYHd/08wOLFRg\nkVhmzw6Pw4fHzSGSj3z23AcCq939dXffCUwFhtVb5yJgpru/CeDu7yQbUyS+mTNhwACduCSlIZ9y\n7wW8lfV8Xea1bEcAXc3sv81ssZl9s6EvZGZXmlm1mVXX1NS0LLFIBO++C088oSEZKR1JfaBaCZwA\nnAucA1xnZkfUX8ndJ7l7lbtXde/ePaG3Fim8OXOgrk7lLqWjyTF3YD3QO+v5IZnXsq0DNrv7NmCb\nmT0JHAO8kkhKkchmzgxHyBx7bOwkIvnJZ8/9eaCfmfU1s/bAKGBuvXXmAKeYWaWZdQJOBF5ONqpI\nHB98AI88EvbazWKnEclPk3vu7l5rZlcDC4AK4C53X25mYzPLJ7r7y2b2ELAUqAPucPdlhQwu0lbm\nzYOdOzUkI6XF3D3KG1dVVXl1dXWU9xZpjpEj4S9/gfXrda9Uic/MFrt7VVPr6UdVpBE7dsD8+TBs\nmIpdSot+XEUa8dhjsHVrKHeRUqJyF2nE7Nmw777wD/8QO4lI86jcRXKoq4O5c2HIEOjQIXYakeZR\nuYvk8NxzsHEjnH9+7CQizadyF8lh9myorIShQ2MnEWk+lbtIDrNnw6BB0KVL7CQizadyF2nAypWw\napWGZKR0qdxFGjBnTnj8+tfj5hBpKZW7SANmzw630+vdu+l1RYqRyl2kno0bw+30dOKSlDKVu0g9\nDz4I7ip3KW0qd5F65syBww6Do46KnUSk5VTuIlm2bQvXbh82TNdul9KmchfJsnAhfPyxhmSk9Knc\nRbLMmQNdu8Kpp8ZOItI6KneRjNra8GHqueeGyw6IlDKVu0jG00/D5s0akpF0ULmLZMyZA+3bwznn\nxE4i0noqdxHCce1z5sCZZ0LnzrHTiLSeyl0EWLECXntNFwqT9FC5ixCuJQNw3nlxc4gkReUuQij3\nk06Cnj1jJxFJhspdyt66dVBdrSEZSReVu5S9uXPDow6BlDRRuUvZmz0b+veHL3whdhKR5Kjcpaxt\n2QKPP64hGUkflbuUtfnzw2UHNCQjaaNyl7I2ezb06AEnnhg7iUiyVO5Stj7+GP7857DXvpd+EyRl\n8vqRNrPBZrbKzFab2bhG1vuymdWa2f9JLqJIYTz8cLg5x/DhsZOIJK/JcjezCmACMAQYAIw2swE5\n1rsRWJh0SJFCmDkTunSBQYNiJxFJXj577gOB1e7+urvvBKYCDX389D3gAeCdBPOJFMSuXeH49vPO\nC1eCFEmbfMq9F/BW1vN1mdf+xsx6Ad8AfpNcNJHCefJJeO89uOCC2ElECiOpj5FuAa5197rGVjKz\nK82s2syqa2pqEnprkeabORM6ddK12yW98rmZ2Hqgd9bzQzKvZasCplq4XXw3YKiZ1br77OyV3H0S\nMAmgqqrKWxpapDXq6mDWLBg6FDp2jJ1GpDDyKffngX5m1pdQ6qOAi7JXcPe+u+fN7G7gwfrFLlIs\nnn0W3n5bQzKSbk2Wu7vXmtnVwAKgArjL3Zeb2djM8okFziiSqJkzw4eo554bO4lI4eR1j3d3nwfM\nq/dag6Xu7t9qfSyRwnCHBx6As86C/faLnUakcHRenpSV6mp44w0YMSJ2EpHCUrlLWZk2Ddq101Ug\nJf1U7lI23GH69HD4Y5cusdOIFJbKXcrGokXw1lswcmTsJCKFp3KXsjFtGnTooGu3S3lQuUtZqKuD\nGTNg8GAdJSPlQeUuZeGZZ2D9eg3JSPlQuUtZmDYN9t47XAVSpByo3CX1PvkkDMkMGQKdO8dOI9I2\nVO6Seo89Bhs3wsUXx04i0nZU7pJ6994L+++va8lIeVG5S6pt2xYuFDZyZBhzFykXKndJtdmzQ8Ff\ncknsJCJtS+UuqXbffdCnD5xySuwkIm1L5S6ptXEjLFwY9tr30k+6lBn9yEtqTZ0azkzVkIyUI5W7\npNa998IJJ8CRR8ZOItL2VO6SSkuWwAsvwKWXxk4iEofKXVLpjjvCfVI1JCPlSuUuqfPRR+EomeHD\n4YADYqcRiUPlLqnzxz/Cli1wxRWxk4jEo3KX1Jk8Gf7u72DQoNhJROJRuUuqrFwJf/kLfOc7YBY7\njUg8KndJlTvvhMpKGDMmdhKRuFTukho7dsCUKeGGHAcdFDuNSFwqd0mNadOgpgbGjo2dRCQ+lbuk\ngjvcems4G/Xss2OnEYmvMnYAkSQ89VQ4I3XiRH2QKgLac5eUuOUW6NpVlxsQ2U3lLiVv7VqYNQuu\nvBI6dYqdRqQ4qNyl5E2YEIZirroqdhKR4pFXuZvZYDNbZWarzWxcA8svNrOlZvaSmT1tZsckH1Xk\ns7ZuDWekDh8OvXvHTiNSPJosdzOrACYAQ4ABwGgzG1BvtTXA6e5+FPAzYFLSQUUaMnFiuI7M978f\nO4lIcclnz30gsNrdX3f3ncBUYFj2Cu7+tLu/n3n6LHBIsjFFPmv7dvjVr8KhjyeeGDuNSHHJp9x7\nAW9lPV+XeS2Xy4H5DS0wsyvNrNrMqmtqavJPKdKAyZNh0ya47rrYSUSKT6IfqJrZGYRyv7ah5e4+\nyd2r3L2qe/fuSb61lJmPP4abboLTT4dTT42dRqT45HMS03og+6OqQzKvfYqZHQ3cAQxx983JxBNp\n2O9+Bxs2hPukishn5bPn/jzQz8z6mll7YBQwN3sFM+sDzAQudfdXko8pssfOnXDDDfCVr8AZZ8RO\nI1Kcmtxzd/daM7saWABUAHe5+3IzG5tZPhG4HjgAuN3Cud+17l5VuNhSzn77W3jzTZg0SZcaEMnF\n3D3KG1dVVXl1dXWU95bStWVLuMvSscfCww+r3KX8mNnifHaedYaqlJRf/ALeey8cAqliF8lN5S4l\nY82acFnfMWPCnruI5KZyl5Lxox9BRQX8/Oexk4gUP5W7lISnn4apU+EHP4BejZ1CJyKAyl1KwM6d\ncMUV4cJg//ZvsdOIlAbdiUmK3g03wIoV8OCD0Llz7DQipUF77lLUVqwIY+yjR8O558ZOI1I6VO5S\ntOrqwnBM587hNnoikj8Ny0jR+q//Ch+kTpkCBx4YO41IadGeuxSlRYtg3Dg4/3zd9FqkJVTuUnTe\nfx8uvBAOOQTuuktnooq0hIZlpKi4w+WXw/r18D//A127xk4kUppU7lJUbr4ZZs0K147RrfNEWk7D\nMlI0ZswIJymNGKEbXou0lspdisJTT4UPTv/+7+GeezTOLtJaKneJ7pVXYNgw6NMH5syBvfeOnUik\n9KncJaqVK2HQINhrL5g/Hw44IHYikXRQuUs0y5bB6aeHM1EffxwOPzx2IpH0ULlLFIsXhz32ykp4\n4gn44hdjJxJJF5W7tLnp0+HUU2GffUKx9+8fO5FI+qjcpc3U1cFPfhLOPj3uOHjuuXCzaxFJnk5i\nkjaxYUM48/Shh8LjhAnQoUPsVCLppT13KbipU+FLXwpDMLffDpMnq9hFCk3lLgXz+utwwQXhRhtH\nHAEvvgjf/a5OUBJpCyp3SdyHH4bL9R55JCxYAL/4RbgI2BFHxE4mUj405i6Jee89GD8ebr01zH/z\nm6HYe/WKnUyk/KjcpdVWroRJk8JY+tatcN55cN118OUvx04mUr5U7tIi778frgNz551hyKWyMlzN\ncdw4OPro2OlEROUueXvjjTCGPmsWPPoo1NZCv35w440wZgz06BE7oYjspnKXBrnD6tXwzDPhJtWP\nPAKvvRaWff7z4Xrrw4eHoRcd/SJSfPIqdzMbDNwKVAB3uPsN9ZZbZvlQYDvwLXd/IeGsUiCbN4fL\n7q5cCUuXhmnJkvA6QOfO4QJf11wDZ50VjoJRoYsUtybL3cwqgAnA2cA64Hkzm+vuK7JWGwL0y0wn\nAr/JPEpEdXVhbLymBt55J5wlumFDuD/p2rVhmGXNmnBky24dO8JRR8E3vgEDB8LJJ4cyr6iI9tcQ\nkRbIZ899ILDa3V8HMLOpwDAgu9yHAfe4uwPPmlkXM+vp7m8nnrhEucMnn+yZamvD465dYX7XrjDt\n3Lln2rFjz/Txx/DRR2Havj1MW7fumT78MExbtoRCf++9MP/JJ5/N0rEjHHpomKqqwvHnu6fDD1eR\ni6RBPuXeC3gr6/k6PrtX3tA6vYDEy33BAviXf9nz3L3h9bJfb2jePfd8ruW7p7q6z87X1TU87S7z\nXDlbo6IiXFlx331h//3D1LVrGBPv2hU+9zno3j1MBx4IBx8MPXtCly4aVhFJuzb9QNXMrgSuBOjT\np0+LvsZ++4XrlHz66+Z6v8bnzXLP51puFu4alD2/+3lFxWfn99orzDc0tWsXDiGsrAzzu6cOHaB9\n+/DYoUO47VyHDtCpU9jr7tgxzHfooJIWkYblU+7rgd5Zzw/JvNbcdXD3ScAkgKqqqhbty558cphE\nRCS3fK4t8zzQz8z6mll7YBQwt946c4FvWnAS8IHG20VE4mlyz93da83samAB4VDIu9x9uZmNzSyf\nCMwjHAa5mnAo5GWFiywiIk3Ja8zd3ecRCjz7tYlZ8w5clWw0ERFpKV3yV0QkhVTuIiIppHIXEUkh\nlbuISAqp3EVEUsi8EOfF5/PGZjXA2hb+8W7AuwnGSUqx5oLizaZczaNczZPGXIe6e/emVopW7q1h\nZtXuXhU7R33FmguKN5tyNY9yNU8559KwjIhICqncRURSqFTLfVLsADkUay4o3mzK1TzK1Txlm6sk\nx9xFRKRxpbrnLiIijSjacjezEWa23MzqzKyq3rIfmtlqM1tlZufk+POfM7OHzezVzGPXAmScZmYv\nZqY3zOzFHOu9YWYvZdarTjpHA+/3UzNbn5VtaI71Bme24WozG9cGuf7TzFaa2VIzm2VmXXKs1ybb\nq6m/f+YS1rdlli81s+MLlSXrPXub2eNmtiLz8/9PDawzyMw+yPr+Xl/oXFnv3ej3JtI265+1LV40\nsw/N7J/rrdMm28zM7jKzd8xsWdZreXVR4r+P7l6UE3Ak0B/4b6Aq6/UBwBKgA9AXeA2oaODP3wSM\ny8yPA24scN6bgetzLHsD6NaG2+6nwA+aWKcis+0+D7TPbNMBBc71VaAyM39jru9JW2yvfP7+hMtY\nzwcMOAlY1Abfu57A8Zn5zsArDeQaBDzYVj9PzfnexNhmDXxfNxKOBW/zbQacBhwPLMt6rckuKsTv\nY9Huubv7y+6+qoFFw4Cp7r7D3dcQriE/MMd6UzLzU4DzC5M07K0AI4H7C/UeBfC3G5+7+05g943P\nC8bdF7p7bebps4Q7dsWSz9//bzd+d/dngS5m1rOQodz9bXd/ITP/V+Blwv2IS0Wbb7N6zgRec/eW\nniDZKu7+JPBevZfz6aLEfx+Lttwbketm3PX18D13g9oI9ChgplOBTe7+ao7lDjxiZosz95FtC9/L\n/Lf4rhz/Dcx3OxbKtwl7eA1pi+2Vz98/6jYys8OA44BFDSz+Sub7O9/MvthWmWj6exP752oUuXey\nYm2zfLoo8e3WpjfIrs/MHgEOamDRj919TlLv4+5uZi06LCjPjKNpfK/9FHdfb2YHAg+b2crMv/At\n1lgu4DfAzwi/iD8jDBl9uzXvl0Su3dvLzH4M1AK/z/FlEt9epcbM9gUeAP7Z3T+st/gFoI+7b818\nnjIb6NdG0Yr2e2PhNqBfB37YwOKY2+xvWtNFzRW13N39rBb8sbxuxg1sMrOe7v525r+F7xQio5lV\nAhcAJzTyNdZnHt8xs1mE/4K16hci321nZpOBBxtYlO92TDSXmX0L+BpwpmcGGxv4GolvrwYkduP3\npJlZO0Kx/97dZ9Zfnl327j7PzG43s27uXvBrqOTxvYmyzTKGAC+4+6b6C2JuM/LrosS3WykOy8wF\nRplZBzPrS/jX97kc643JzI8BEvufQD1nASvdfV1DC81sHzPrvHue8KHisobWTUq9Mc5v5Hi/fG58\nnnSuwcD/Bb7u7ttzrNNW26sob/ye+fzmTuBld/91jnUOyqyHmQ0k/B5vLmSuzHvl871p822WJef/\noGNts4x8uij538dCf3rc0olQSuuAHcAmYEHWsh8TPlleBQzJev0OMkfWAAcAjwKvAo8AnytQzruB\nsfVeOxiYl5n/POGT7yXAcsLwRKG33b3AS8DSzA9Iz/q5Ms+HEo7GeK2Ncq0mjCu+mJkmxtxeDf39\ngbG7v5+EIz4mZJa/RNZRWwXMdAphOG1p1nYaWi/X1Zlts4TwwfRXCp2rse9N7G2Wed99CGW9f9Zr\nbb7NCP+4vA3syvTX5bm6qNC/jzpDVUQkhUpxWEZERJqgchcRSSGVu4hICqncRURSSOUuIpJCKncR\nkRRSuYuIpJDKXUQkhf4/BiFaifYsob0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1142e5208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_values = np.linspace(-10, 10, 100)\n",
    "y_values = 1/(1 + np.exp(-1*X_values))\n",
    "plt.plot(X_values, y_values, color='Blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where this function comes from - log odds (logit)\n",
    "This function comes from the concept of the odds ratio in probability, which for a probability p, of a given event, is defined as:\n",
    "\\begin{equation*}\n",
    "odds(p) = \\frac {p} {(1-p)}\n",
    "\\end{equation*}\n",
    "\n",
    "For example, if you hear someone say the odds of an event are 1:1, then that means there is a 50% chance of that event happening.\n",
    "\n",
    "By taking the natural logarithm (ln) of both sides, we can define the **logit** or **log odds** function. \n",
    "\\begin{equation*}\n",
    "logit(p) = ln \\frac {p} {(1-p)}\n",
    "\\end{equation*}\n",
    "\n",
    "The **logistic function** which we defined earlier is actually the **inverse** of the **logit** function. \n",
    "\n",
    "#### Applying the Logistic Function to a Weighted Sum of Features\n",
    "The logistic regression model is based on the following equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "logit \\ (p(y=1 \\ | \\ x)) = w_0 x_0 + w_1 x_1 + ... + w_m x_m = \\sum_{i=0}^n w_m x_m = w^T x\n",
    "\\end{equation*}\n",
    "\n",
    "Where:\n",
    "- $ x = (x_0, x_1, ..., x_n) $ represents the input in the form of a vector of features given to the model.\n",
    "- $ p(y=1 \\ | \\ x) $ is the conditional probability that given an input $ x $, the data point belongs to the class $ y = 1 $. For our case, this is probability that given the values of variables like Gender, Age, Class, etc. that a person survived the Titanic.\n",
    "- $ w = (w_0, w_1, ..., w_n) $ represents a vector of numerical weights that will be used to compute the weighted sum of the inputs.\n",
    "- $ w^T x $ is just a short way to write the weighted sum of the features using matrix notation. Since x and w are actually just one dimensional matrices (vectors), this is basically equivalent to taking the dot product of x and w (multiply each feature by each corresponding weight and add them up!).\n",
    "\n",
    "By applying the **logistic function** to **both sides** of the equation above we get the following result:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(y=1 \\ | \\ x) = \\phi(w^T x)\n",
    "\\end{equation*}\n",
    "\n",
    "This basically means that the output that comes from applying this function to a weighted sum of the features gives us the probability that an observation belongs to the class 1. In our case, the raw output from the model when making a prediction will give us the probability that a person survived the titanic. Hence, if this probability is **greater than our equal to 0.5** (**50% or more chance** they survived) then our model predicts that the person survived. Otherwise, the model predicts that the person did not survive.\n",
    "\n",
    "#### How to logistic regression generates class predictions\n",
    "The class predictions that come out of a logistic regression model are based on the following equation:\n",
    "\n",
    "$$\n",
    "y =\n",
    "  \\begin{cases}\n",
    "    1 &\\text{if } \\phi(w^T x) \\ge 0.5\\\\\n",
    "    0 &\\text{if } \\phi(w^T x) < 0.5\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "#### How the model learns the weights\n",
    "The process of \"learning\" in logistic regression involves determining the **optimal weights** in the weight vector to **minimize** a cost function defined below for a weight vector $w$:\n",
    "\n",
    "\\begin{equation*}\n",
    "Cost(w) = \\sum_{i=0}^n \\frac {1} {2} (\\hat y_i - y_i)^2 + \\frac {\\lambda} {2} \\sum_{j=1}^m |w_{j}^2|\n",
    "\\end{equation*}\n",
    "\n",
    "Note that the first part of this function is essentially half the sum of squared errors between the predicted values (classes) denoted as $\\hat y_i$ for the ith observation in the data and the actual values, denoted as $y_i$ for the ith observation in the data. For a given weight vector $w$ and an input $x_i$ we know that $\\hat y_i = \\phi(w^T x_i)$ based on the equation for the logistic regression algorithm. The second half of the equation, $ \\frac {\\lambda} {2} \\sum_{j=1}^m w_{j}^2 $ is known as the regularization term and is used to control the values of weights by adding a constant times the square of the magnitude of the weight vector to the cost function.\n",
    "\n",
    "#### Visualization to summarize what we have covered above\n",
    "<img src=\"logistic_regression.png\" style=\"height:300px; width:700px;\"/>\n",
    "\n",
    "Image Credit: Sebastian Raschka, url: https://sebastianraschka.com/faq/docs/logisticregr-neuralnet.html\n",
    "#### Why do we call it a cost function? What is the purpose of a cost function?\n",
    "The cost function is used to quantify how bad our classifier is doing at making predictions on training data. Cost functions are sometimes called **loss functions** but the idea is the same - both tell us how bad our classifier is doing and in general, we want to **minimize** the values of these functions, which is like minimizing a cost or loss. In algorithms that involve learning numerical weights, cost functions give our algorithms a metric for which to learn the optimal weights.\n",
    "\n",
    "#### How do we actually minimize the cost function\n",
    "The process of minimizing the cost function involves an optimization process known as **gradient descent**. The idea behind gradient descent is that we can exploit the fact that our cost function is quadratic by looking at the direction in which our cost function is changing with respect to the weights (basically a derivative) and update our weights in a manner that causes our cost function to continue decreasing until we reach the global minimum, or the vertex of our quadratic curve. The mathematical details of this optimization method are out of the scope of this workshop, but are commonly used in deep learning and if you are interested, you can read more here: http://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html\n",
    "\n",
    "#### An intuitive visualization of gradient descent\n",
    "<img src=\"gradient_descent.png\" style=\"height:450px; width:600px;\"/>\n",
    "\n",
    "Image Credit: Sebastian Raschka, url: https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html\n",
    "\n",
    "#### What is regularization? \n",
    "Regularization is a common practice in machine learning used to control **overfitting** - the situation in which a model becomes too constrained to the patterns in the training data and fails to generalize to unseen data. Regularization works by adding a term that involves the magnitude of the weight vector to the cost function. Doing so penalizes larger weights and prevents any one feature from receiving too much weight in the model. There are two common types of regularization used for logistic regression and those are L2 and L1 regularization. \n",
    "\n",
    "- **L2 Regularization**: This form of regularization is the same as regularization used in the original cost function above and involves the regularization term $ \\frac {\\lambda} {2} \\sum_{j=1}^m w_{j}^2 $.\n",
    "- **L1 Regularization**: This form of regularization involves a different regularization term $ \\frac {\\lambda} {2} \\sum_{j=1}^m |w_{j}| $. The difference here is that we are summing the absolute values of each weight and multipyling by a constant. L1 regularization tends to produce weight vectors where less important features have weights that are close to, if not exactly zero. For this reason, this type of regularization is often good for feature selection.\n",
    "\n",
    "The constant related to logistic regression that we use to optimize the amount of regularization is referred to as $ C $ and is related to the regularization parameter $ \\lambda $ by the following equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "C = \\frac {1} {\\lambda}\n",
    "\\end{equation*}\n",
    "\n",
    "When training a logistic regression model in SciKit-Learn we can pass in values for C and specify the type of regularization that we want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will train a logistic regression model with a C value of 1 with L2 regularization.\n",
    "lr_pipeline = Pipeline([('tfidf', TfidfVectorizer()), ('lr', LogisticRegression(C=1, penalty='l2'))])\n",
    "lr_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.90      0.90      7367\n",
      "          1       0.91      0.89      0.90      7633\n",
      "\n",
      "avg / total       0.90      0.90      0.90     15000\n",
      "\n",
      "Accuracy on test set: 89.64666666666666 %\n"
     ]
    }
   ],
   "source": [
    "pred = lr_pipeline.predict(X_test)\n",
    "print(classification_report(pred, y_test))\n",
    "print('Accuracy on test set: {} %'.format(100*accuracy_score(pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the Optimal Regularization Parameters using Grid Search\n",
    "We can improve the performance of our logistic regression model by optimizing the regularization parameters - the value for C and the type of regularization (L1 or L2). Since we have multiple parameters to optimize, we can use a procedure called **grid search** to test all the different combinations of parameters for a given range of values and then find the parameters for which the model performs the best using cross-validation. SciKit-Learn has a module known as **GridSearchCV** that we can use for this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV # Module for grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of Parameters for GridSearchCV\n",
    "The GridSearchCV constructor takes the following parameters:\n",
    "- **estimator** - specifies the algorithm that we want to perform grid search on\n",
    "- **param_grid** - a dictionary or list of dictionaries with the parameter values that we want to test\n",
    "- **scoring** - the evaluation metric used for cross validation\n",
    "- **cv** - the number of cross validation folds to use\n",
    "- **n_jobs** - the number of parallel threads to use in the training and testing process\n",
    "- **verbose** - tells the program how verbose the output should be (possible values are 0, 1, or 2)\n",
    "\n",
    "Note that the code below takes at least 1 minute and 30 seconds to run and may take longer depending on the your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  18 out of  18 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=6,\n",
       "       param_grid={'lr__penalty': ['l1', 'l2'], 'lr__C': [3.0, 4.0, 5.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'lr__penalty':['l1', 'l2'], 'lr__C': [3.0, 4.0, 5.0]}\n",
    "lr_pipeline = Pipeline([('tfidf', TfidfVectorizer()), ('lr', LogisticRegression())])\n",
    "lr_grid = GridSearchCV(estimator=lr_pipeline, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=6, \n",
    "                    verbose=1)\n",
    "lr_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the best score for our grid search as demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90072"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our best model from the GridSearchCV achieves over 90% 3-fold cross validation accuracy. We can view the scores for each combination of parameters as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/model_selection/_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.89300, std: 0.00117, params: {'lr__C': 3.0, 'lr__penalty': 'l1'},\n",
       " mean: 0.90006, std: 0.00096, params: {'lr__C': 3.0, 'lr__penalty': 'l2'},\n",
       " mean: 0.89134, std: 0.00139, params: {'lr__C': 4.0, 'lr__penalty': 'l1'},\n",
       " mean: 0.90064, std: 0.00031, params: {'lr__C': 4.0, 'lr__penalty': 'l2'},\n",
       " mean: 0.89010, std: 0.00230, params: {'lr__C': 5.0, 'lr__penalty': 'l1'},\n",
       " mean: 0.90072, std: 0.00045, params: {'lr__C': 5.0, 'lr__penalty': 'l2'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_grid.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the best model from our grid search and use it to make predictions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_grid.best_estimator_.predict([\"Batman vs Superman was a complete disappointment!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning for Sentiment Analysis\n",
    "Deep learning is a subfield of machine learning that focuses on the use of algorithms involving neural networks. Neural networks are biologically-inspired computational structures with neurons in a graph with connections that are analagous to synaptic connections in neural systems.\n",
    "\n",
    "<img src=\"neural_net.png\" style=\"height:300px; width:550px;\"/>\n",
    "\n",
    "As we can see in the image above, neural networks have a series of layers, described in detail below:\n",
    "\n",
    "- **input layer** - this layer contains the inputs or features that go into the deep learning model.\n",
    "- **hidden layers** - these layers between the input and output contain **weighted connections** that the model **learns** in order to make predictions.\n",
    "- **output layer** - this layer contains the outputs or predictions of the deep learning model.\n",
    "\n",
    "Each connection in the network has a numerical weight attached to it and the model learns the weights similar to how the logistic regression algorithm determines the optimal weight parameters. Each neuron or node in the graph also has an associated **activation function**. The **sigmoid or logistic function** that we defined in the logistic regression section is a commonly used activation function for neural networks.\n",
    "\n",
    "The process of computing the values at each neuron in the network is driven by matrix multiplication. Each set of weights connecting adjacent layers can be represented by a weight matrix. This weight matrix can be multiplied with the values of the neurons in the previous layer and then after applying the **activation function** to the result of the matrix multiplication, the results represent the values of the neurons in the next layer.\n",
    "\n",
    "This neural network architecture that we are referring to is often called the Multi-Layered Perceptron (MLP) architecture.\n",
    "\n",
    "Like logistic regression, neural networks also involve learning the weights to optimize an objective cost function.\n",
    "\n",
    "#### Visualizing the concept of weights and activations\n",
    "<img src=\"neural_net_math.png\" style=\"height:500px; width:700px;\"/>\n",
    "\n",
    "Image Credit: https://matrices.io/deep-neural-network-from-scratch/\n",
    "\n",
    "#### How does the model learn the weights?\n",
    "Neural networks use the same general gradient descent procedure used in logistic regression to optimize the weights. However, in this case, each weight is updated using a more general approach known as backpropagation, which involves iteratively applying the chain rule to figure out how much the cost function is changing with respect to each weight and then updating each weight accordingly based on a learning rate parameter. \n",
    "\n",
    "#### Training a Deep Learning Sentiment Classifier\n",
    "We can also create pipelines with TF-IDF and deep learning models using SciKit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=Tr...=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_neural_net = Pipeline([('vectorizer', tfidf), ('mlp', MLPClassifier(hidden_layer_sizes=(30, 30)))])\n",
    "tfidf_neural_net.fit(X_train, y_train)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = tfidf_neural_net.predict(X_test)\n",
    "print(classification_report(pred, y_test))\n",
    "print('Accuracy on test set: {} %'.format(100*accuracy_score(pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the test above, we can see that our deep learning classifier scores just over 89 % accuracy, which is almost as good as the optimized logistic regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Function to Predict Sentiment of Reviews\n",
    "Now that we have trained some classifiers that can predict the sentiment of reviews with high levels of accuracy, we can take our best classifier, fit it on the entire dataset, and define a function that takes in a review as a string and labels the review as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('tfidf', TfidfVectorizer()), ('lr', LogisticRegression(C=5.0))])\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "def analyze_sentiment(string):\n",
    "    if pipeline.predict([string]) == [1]:\n",
    "        return \"Positive review!\"\n",
    "    else:\n",
    "        return \"Negative review!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative review!'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_sentiment(\"There are movies that are bad. There are movies that are so bad they're good. And then there's Troll 2, a movie that's so bad it defies comprehension.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive review!'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_sentiment(\"The Death of Stalin was the most realistic and well-directed historical film of 2018.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive review!'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_sentiment(\"That movie was really good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative review!'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_sentiment(\"That movie was really not good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact Info\n",
    "**Amol Mavuduru**\n",
    "\n",
    "**Email**: axm168430@utdallas.edu"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
